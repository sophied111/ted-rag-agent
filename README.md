# TED Talk RAG Assistant

A Retrieval-Augmented Generation (RAG) system for querying TED Talks using semantic search and AI-powered answers. The system strictly answers questions based only on the TED Talks dataset without using external knowledge.

## ğŸ¯ Features

- **Semantic Search**: Vector-based retrieval using OpenAI embeddings
- **Smart Chunking**: Sentence-aware text splitting with configurable overlap
- **Rich Metadata**: 18 metadata fields stored per chunk for comprehensive context
- **AI-Powered Answers**: GPT-4 generates responses strictly from retrieved context
- **RESTful API**: Deployed on Vercel with CORS support
- **Configuration Management**: Optimized chunking parameters from hyperparameter tuning

## ğŸ“‹ Table of Contents

- [Architecture](#architecture)
- [Installation](#installation)
- [Configuration](#configuration)
- [Data Preparation](#data-preparation)
- [Indexing TED Talks](#indexing-ted-talks)
- [API Deployment](#api-deployment)
- [API Usage](#api-usage)
- [Development](#development)
- [Troubleshooting](#troubleshooting)

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TED Talks CSV      â”‚
â”‚  (ted_talks_en.csv) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ embed_and_index.py  â”‚ â† Preprocessing & Chunking
â”‚                     â”‚ â† Embedding Generation
â”‚                     â”‚ â† Pinecone Upload
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Pinecone   â”‚
     â”‚ Vector DB   â”‚
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   Vercel API     â”‚
     â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
     â”‚   â”‚prompt.py â”‚â—„â”€â”€â”¼â”€â”€ POST /api/prompt
     â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
     â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
     â”‚   â”‚stats.py  â”‚â—„â”€â”€â”¼â”€â”€ GET /api/stats
     â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
     â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
     â”‚   â”‚index.htmlâ”‚â—„â”€â”€â”¼â”€â”€ GET /
     â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Installation

### Prerequisites

- Python 3.8+
- OpenAI API key (via LLMOD AI)
- Pinecone account and API key
- Vercel account (for deployment)
- TED Talks dataset CSV file

### Install Dependencies

```bash
pip install pandas openai pinecone-client python-dotenv
```

## âš™ï¸ Configuration

### Environment Variables

Create a `.env` file in the project root:

```bash
# API Keys (Required)
OPENAI_API_KEY=your-llmod-openai-api-key
PINECONE_API_KEY=your-pinecone-api-key
PINECONE_INDEX=ted-rag-index

# Data Configuration
TED_CSV_PATH=ted_talks_en.csv

# Indexing Options (Optional)
SAMPLE_N=0                # Number of talks to index (0 = all)
FORCE_REEMBED=false       # Set to 'true' to overwrite existing vectors
EMBEDDING_API_KEY=        # Optional: separate key for embeddings
```

### Best Configuration File

The system requires a `best_config.json` file that defines optimal chunking parameters. This file should be generated by running hyperparameter experiments first.

**Example `best_config.json`:**

```json
{
  "scheme_id": "cs512_ol10",
  "chunk_size": 512,
  "overlap_ratio": 0.1,
  "top_k": 10,
  "mean_score": 1.0
}
```

**Parameters:**
- `scheme_id`: Unique identifier for the chunking configuration
- `chunk_size`: Maximum tokens per chunk (â‰¤ 2048)
- `overlap_ratio`: Overlap between consecutive chunks (0.0 - 0.3)
- `top_k`: Number of relevant chunks to retrieve per query
- `mean_score`: Evaluation score from hyperparameter tuning

## ğŸ“Š Data Preparation

### Required CSV Columns

Your `ted_talks_en.csv` must include these 18 columns:

| Column | Type | Description |
|--------|------|-------------|
| `talk_id` | string | Unique identifier for the talk |
| `title` | string | Talk title |
| `transcript` | string | Full transcript text |
| `speaker_1` | string | Primary speaker name |
| `all_speakers` | string | All speakers (comma-separated) |
| `topics` | string | Topics/tags (comma-separated) |
| `description` | string | Talk description |
| `occupations` | string | Speaker occupations |
| `about_speakers` | string | Speaker biography |
| `views` | string/int | View count |
| `recorded_date` | string | Recording date |
| `published_date` | string | Publication date |
| `event` | string | TED event name |
| `native_lang` | string | Original language code |
| `available_lang` | string | Available translations |
| `duration` | string/int | Duration in seconds |
| `related_talks` | string | Related talk IDs |
| `url` | string | TED.com URL |

### CSV Format Example

```csv
talk_id,title,transcript,speaker_1,topics,description,...
2817,"Your brain hallucinates your conscious reality","Right now...","Anil Seth","brain,neuroscience","Neuroscientist Anil Seth...",...
```

## ğŸ”§ Indexing TED Talks

### Step 1: Prepare Configuration

Ensure you have `best_config.json` in the project root. If not, create it manually or run hyperparameter experiments.

### Step 2: Run Indexing Script

**Index all talks:**

```bash
python embed_and_index.py
```

**Index a sample (for testing):**

```bash
# Windows PowerShell
$env:SAMPLE_N="100"
python embed_and_index.py

# Linux/macOS
SAMPLE_N=100 python embed_and_index.py
```

**Force re-indexing (overwrites existing data):**

```bash
# Windows PowerShell
$env:FORCE_REEMBED="true"
python embed_and_index.py

# Linux/macOS
FORCE_REEMBED=true python embed_and_index.py
```

### What Happens During Indexing

1. **Loads Configuration**: Reads `best_config.json`
2. **Loads TED Talks**: Reads CSV and validates columns
3. **Preprocesses Text**: 
   - Removes TED artifacts `(Laughter)`, `(Applause)`, etc.
   - Removes timestamps `[00:00]`
   - Normalizes whitespace and punctuation
4. **Chunks Transcripts**: 
   - Sentence-aware splitting
   - Respects token limits
   - Creates overlapping chunks
5. **Generates Embeddings**: Uses `text-embedding-3-small` (1536 dimensions)
6. **Uploads to Pinecone**: Batches of 96 vectors per request
7. **Stores Metadata**: All 18 fields stored alongside vectors

### Expected Output

```
============================================================
TED Talk RAG - Embedding & Indexing
============================================================

Loading best configuration from best_config.json...
âœ“ Loaded configuration: cs512_ol10

Configuration:
  CSV Path: ted_talks_en.csv
  Pinecone Index: ted-rag-index
  Sample Size: All data
  Force Re-embed: false

Chunking Scheme (from best_config.json):
  Scheme ID: cs512_ol10
  Chunk Size: 512 tokens
  Overlap Ratio: 0.1

============================================================
Loading TED Talks Data
============================================================
Loaded 4005 talks

============================================================
Initializing API Clients
============================================================
âœ“ Clients initialized
Index 'ted-rag-index' already exists

============================================================
Embedding & Uploading Chunks
============================================================

[1/1] Processing scheme: cs512_ol10
  Created 45623 chunks, uploading to Pinecone...
  âœ“ Uploaded to namespace: cs512_ol10

============================================================
âœ“ Embedding and Indexing Complete!
============================================================

Processed scheme: cs512_ol10
Total talks embedded: 4005
Namespace: cs512_ol10
```

## ğŸŒ API Deployment

### Deploy to Vercel

1. **Install Vercel CLI:**

```bash
npm install -g vercel
```

2. **Deploy:**

```bash
# First time deployment
vercel

# Production deployment
vercel --prod
```

3. **Set Environment Variables in Vercel Dashboard:**

   - Go to your project settings
   - Navigate to "Environment Variables"
   - Add: `OPENAI_API_KEY`, `PINECONE_API_KEY`, `PINECONE_INDEX`

### Local Development

```bash
vercel dev
```

Access at: `http://localhost:3000`

## ğŸ“¡ API Usage

### Endpoints

#### 1. **POST /api/prompt** - Query the RAG System

**Request:**

```bash
curl -X POST https://your-app.vercel.app/api/prompt \
  -H "Content-Type: application/json" \
  -d '{"question": "Find a TED talk about how your brain hallucinates reality"}'
```

**Request Body:**

```json
{
  "question": "Your natural language question here"
}
```

**Response:**

```json
{
  "response": "The TED talk you're looking for is 'Your brain hallucinates your conscious reality' by Anil Seth...",
  "context": [
    {
      "talk_id": "2817",
      "title": "Your brain hallucinates your conscious reality",
      "chunk": "Right now, billions of neurons in your brain are working together...",
      "score": 0.8543
    }
  ],
  "Augmented_prompt": {
    "System": "You are a TED Talk assistant that answers questions...",
    "User": "Use ONLY the following TED dataset context..."
  }
}
```

**Query Types Supported:**

1. **Fact Retrieval**: "Find the TED talk where [speaker] discusses [topic]"
2. **Multi-Result Listing**: "List all TED talks about artificial intelligence"
3. **Key Idea Extraction**: "What are the main ideas in talks about creativity?"
4. **Recommendations**: "Recommend talks similar to [talk title]"

#### 2. **GET /api/stats** - Get Configuration

**Request:**

```bash
curl https://your-app.vercel.app/api/stats
```

**Response:**

```json
{
  "chunk_size": 512,
  "overlap_ratio": 0.1,
  "top_k": 10
}
```

#### 3. **GET /** - Web Interface

Access the interactive API documentation at the root URL.

### Python Example

```python
import requests

# Query the RAG system
response = requests.post(
    "https://your-app.vercel.app/api/prompt",
    json={"question": "What TED talks discuss brain plasticity?"}
)

result = response.json()

# Display answer
print("Answer:", result["response"])

# Display sources
print(f"\nFound {len(result['context'])} relevant talks:")
for ctx in result["context"]:
    print(f"- {ctx['title']} (ID: {ctx['talk_id']}, Score: {ctx['score']:.3f})")
```

### JavaScript Example

```javascript
fetch('https://your-app.vercel.app/api/prompt', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    question: 'Find talks about climate change solutions'
  })
})
.then(res => res.json())
.then(data => {
  console.log('Answer:', data.response);
  console.log('Sources:', data.context);
});
```

## ğŸ› ï¸ Development

### Project Structure

```
.
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ prompt.py           # Main RAG query endpoint
â”‚   â””â”€â”€ stats.py            # Configuration stats endpoint
â”œâ”€â”€ embed_and_index.py      # Indexing script
â”œâ”€â”€ embedding_and_retreival_hyperparameter_experiments.py
â”‚                           # Hyperparameter tuning (optional)
â”œâ”€â”€ best_config.json        # Optimal configuration
â”œâ”€â”€ index.html              # API documentation page
â”œâ”€â”€ vercel.json             # Vercel deployment config
â”œâ”€â”€ ted_talks_en.csv        # TED talks dataset
â”œâ”€â”€ .env                    # Environment variables
â””â”€â”€ README.md               # This file
```

### Key Components

#### `embed_and_index.py`

Main indexing script with these functions:

- `preprocess_text()` - Text cleaning and normalization
- `chunk_text()` - Sentence-aware chunking with overlap
- `build_chunks_for_scheme()` - Creates chunk records with metadata
- `embed_texts()` - Generates embeddings via OpenAI
- `upsert_chunks()` - Uploads vectors to Pinecone

#### `api/prompt.py`

Query endpoint with these functions:

- `load_config()` - Loads `best_config.json`
- `query_index()` - Semantic search in Pinecone
- `build_prompt()` - Constructs augmented prompt with context
- `rag_answer()` - End-to-end RAG pipeline

#### `api/stats.py`

Returns current configuration parameters from `best_config.json`.

### Chunking Strategy

The system uses **sentence-aware chunking** to maintain semantic coherence:

1. **Splits on sentence boundaries** (`.`, `!`, `?`)
2. **Accumulates sentences** until token limit reached
3. **Creates overlaps** by including previous sentences
4. **Falls back to word-level** for oversized sentences

**Example:**

```
Transcript (1200 tokens):
"[Sentence 1: 250 tokens] [Sentence 2: 200 tokens] [Sentence 3: 300 tokens]..."

With chunk_size=512, overlap_ratio=0.1 (51 tokens):

Chunk 0: [S1] [S2] [S3 (partial)]     â†’ 512 tokens
Chunk 1: [S3 (overlap)] [S4] [S5]      â†’ 512 tokens  
Chunk 2: [S5 (overlap)] [S6]...        â†’ 476 tokens
```

### System Prompt

The API enforces strict adherence to the dataset:

```
You are a TED Talk assistant that answers questions strictly and 
only based on the TED dataset context provided to you (metadata 
and transcript passages). You must not use any external 
knowledge, the open internet, or information that is not explicitly 
contained in the retrieved context. If the answer cannot be 
determined from the provided context, respond: I don't know 
based on the provided TED data.
```

## ğŸ› Troubleshooting

### Common Issues

**1. Missing `best_config.json`:**

```
FileNotFoundError: Configuration file 'best_config.json' not found
```

**Solution:** Create the file manually or run hyperparameter experiments to generate it.

**2. Missing Environment Variables:**

```
RuntimeError: Missing required environment variables: OPENAI_API_KEY
```

**Solution:** Set all required environment variables in `.env` or system environment.

**3. Missing CSV Columns:**

```
ValueError: Missing required columns: {'speaker_1', 'topics'}
```

**Solution:** Ensure your CSV has all 18 required columns.

**4. No Results from Queries:**

```
Low scores, talks not found
```

**Solution:** 
- Verify data is indexed: Check Pinecone dashboard for vector count
- Run `python embed_and_index.py` to index data
- Ensure `scheme_id` in `best_config.json` matches indexed namespace

**5. Namespace Mismatch:**

```
Empty results from Pinecone
```

**Solution:** 
- Check `best_config.json` has correct `scheme_id`
- Verify namespace exists in Pinecone: `index.describe_index_stats()`
- Re-run indexing if needed

**6. Rate Limiting:**

```
OpenAI RateLimitError: Rate limit exceeded
```

**Solution:** 
- Add delays between API calls
- Reduce batch size in `upsert_chunks()`
- Upgrade OpenAI API tier

### Debugging Tips

**Check Pinecone Index Status:**

```python
from pinecone import Pinecone
import os

pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
index = pc.Index(os.getenv("PINECONE_INDEX"))

stats = index.describe_index_stats()
print(stats)
```

**Test Embedding Generation:**

```python
from embed_and_index import embed_texts
from openai import OpenAI

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"), base_url="https://api.llmod.ai/v1")
embeddings = embed_texts(client, ["test sentence"])
print(f"Embedding dimension: {len(embeddings[0])}")  # Should be 1536
```

**Test Chunking:**

```python
from embed_and_index import chunk_text

text = "First sentence. Second sentence. Third sentence. Fourth sentence."
chunks = chunk_text(text, max_tokens=20, overlap_ratio=0.2)
print(f"Created {len(chunks)} chunks:")
for i, chunk in enumerate(chunks):
    print(f"  Chunk {i}: {chunk[:50]}...")
```

## ğŸ“ˆ Performance & Costs

### Indexing Costs

- **Model**: `text-embedding-3-small` ($0.02 per 1M tokens)
- **~4000 talks**: ~40,000 chunks Ã— 400 tokens avg = 16M tokens
- **Estimated cost**: ~$0.32 for full dataset

### Query Costs

- **Embedding**: ~$0.000008 per query
- **Chat (GPT-4o-mini)**: ~$0.0001 per query
- **Total**: ~$0.0001 per query

### Pinecone Storage

- **Free tier**: 100,000 vectors (suitable for ~2,500 talks)
- **Paid**: Starting at $0.096 per 1M vectors/month

### Optimization Tips

1. **Use sampling during development**: Set `SAMPLE_N=100` for testing
2. **Enable skip logic**: Don't set `FORCE_REEMBED` unless necessary
3. **Batch efficiently**: Default 96 vectors per upsert is optimal
4. **Cache common queries**: Implement query caching for repeated questions

## ğŸ“ License

This project is for educational purposes as part of an MSc assignment.

## ğŸ™ Acknowledgments

- TED Talks for the dataset
- OpenAI for embedding and chat models
- Pinecone for vector database
- LLMOD AI for API gateway
- Vercel for serverless hosting

---

**Note**: This RAG system is designed to answer questions strictly based on the TED Talks dataset without using external knowledge or internet access.